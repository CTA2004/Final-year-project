{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13264787,"sourceType":"datasetVersion","datasetId":8405860},{"sourceId":13667088,"sourceType":"datasetVersion","datasetId":8649581},{"sourceId":663068,"sourceType":"modelInstanceVersion","modelInstanceId":501707,"modelId":516871},{"sourceId":663167,"sourceType":"modelInstanceVersion","modelInstanceId":501789,"modelId":516953}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install faiss-cpu PyMuPDF","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:45.480376Z","iopub.execute_input":"2025-12-17T10:41:45.480626Z","iopub.status.idle":"2025-12-17T10:41:48.588665Z","shell.execute_reply.started":"2025-12-17T10:41:45.480609Z","shell.execute_reply":"2025-12-17T10:41:48.587904Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.13.1)\nRequirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.7)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:48.590456Z","iopub.execute_input":"2025-12-17T10:41:48.590731Z","iopub.status.idle":"2025-12-17T10:41:51.941580Z","shell.execute_reply.started":"2025-12-17T10:41:48.590710Z","shell.execute_reply":"2025-12-17T10:41:51.940771Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.57.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:51.942686Z","iopub.execute_input":"2025-12-17T10:41:51.943083Z","iopub.status.idle":"2025-12-17T10:41:51.960982Z","shell.execute_reply.started":"2025-12-17T10:41:51.943054Z","shell.execute_reply":"2025-12-17T10:41:51.960054Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3054cf85e33641b8aea125e099d6b0f7"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"!pip install rank_bm25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:51.962744Z","iopub.execute_input":"2025-12-17T10:41:51.963069Z","iopub.status.idle":"2025-12-17T10:41:55.061297Z","shell.execute_reply.started":"2025-12-17T10:41:51.963052Z","shell.execute_reply":"2025-12-17T10:41:55.060492Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rank_bm25) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rank_bm25) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rank_bm25) (2024.2.0)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"bkai-foundation-models/vietnamese-bi-encoder\")\nmodel.save(\"./vietnamese-bi-encoder\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:55.062471Z","iopub.execute_input":"2025-12-17T10:41:55.062879Z","iopub.status.idle":"2025-12-17T10:41:57.557614Z","shell.execute_reply.started":"2025-12-17T10:41:55.062831Z","shell.execute_reply":"2025-12-17T10:41:57.557015Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\nmodel.save(\"./ms-marco-MiniLM-L-12-v2\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:57.558373Z","iopub.execute_input":"2025-12-17T10:41:57.558660Z","iopub.status.idle":"2025-12-17T10:41:58.775491Z","shell.execute_reply.started":"2025-12-17T10:41:57.558638Z","shell.execute_reply":"2025-12-17T10:41:58.774646Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"%%writefile HFLLMAdapter.py\nimport os\nimport faiss\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sentence_transformers import SentenceTransformer\nfrom typing import List\nimport textwrap\n\nclass HFLLMAdapter:\n    def __init__(self, model_id=\"/kaggle/input/qwen_law_instruct_v3/keras/default/2\", device=None):\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if \"cuda\" in self.device else torch.float32,\n            device_map=\"auto\"\n        )\n\n    def generate(self, prompt: str, max_new_tokens: int = 512) -> str:\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.3,\n                do_sample=False\n            )\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:58.776343Z","iopub.execute_input":"2025-12-17T10:41:58.776811Z","iopub.status.idle":"2025-12-17T10:41:58.782108Z","shell.execute_reply.started":"2025-12-17T10:41:58.776783Z","shell.execute_reply":"2025-12-17T10:41:58.781333Z"}},"outputs":[{"name":"stdout","text":"Overwriting HFLLMAdapter.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"%%writefile LegalRetriever.py\nimport re\nimport numpy as np\nimport faiss\nfrom rank_bm25 import BM25Okapi\nfrom typing import List, Tuple, Dict\nfrom sentence_transformers import SentenceTransformer\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\nclass HybridLegalRetriever:\n    def __init__(\n        self,\n        embedder_path: str = \"./vietnamese-bi-encoder\",\n        reranker_model: str = \"./ms-marco-MiniLM-L-12-v2\",\n        top_k: int = 3,\n        batch_size: int = 8,\n    ):\n        # Embedding model\n        self.embedder = SentenceTransformer(embedder_path)\n        self.top_k = top_k\n        self.batch_size = batch_size\n\n        # Reranker (sequence classification model)\n        self.reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model)\n        self.reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model)\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.reranker_model.to(self.device)\n        self.reranker_model.eval()\n\n        # Indexes / storage\n        self.bm25 = None\n        self.faiss_index = None\n        self.text_chunks: List[str] = []\n        self.embeddings = None\n\n    # =========================================================\n    # Build index: BM25 + FAISS\n    # =========================================================\n    def build_index(self, chunks: List[str]):\n        if not chunks:\n            raise ValueError(\"‚ùå Danh s√°ch chunks r·ªóng, kh√¥ng th·ªÉ x√¢y d·ª±ng index.\")\n\n        self.text_chunks = chunks\n\n        # --- BM25 index (tokenize with words) ---\n        print(\"üîπ X√¢y d·ª±ng BM25 index...\")\n        tokenized_corpus = [re.findall(r'\\w+', c.lower()) for c in chunks]\n        self.bm25 = BM25Okapi(tokenized_corpus)\n\n        # --- FAISS index (embedding vectors) ---\n        print(\"üîπ ƒêang t·∫°o embeddings v√† x√¢y d·ª±ng FAISS index...\")\n        embeddings = self.embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n        embeddings = np.asarray(embeddings, dtype=np.float32)\n\n        # normalize and add to index\n        faiss.normalize_L2(embeddings)\n        dim = embeddings.shape[1]\n        self.faiss_index = faiss.IndexFlatIP(dim)\n        self.faiss_index.add(embeddings)\n        self.embeddings = embeddings\n\n        print(f\"‚úÖ Ho√†n t·∫•t! ƒê√£ x√¢y d·ª±ng index cho {len(chunks):,} ƒëo·∫°n lu·∫≠t v·ªõi vector dim = {dim}.\")\n        print(f\"   ‚Ä¢ FAISS vectors: {self.faiss_index.ntotal}\")\n        print(f\"   ‚Ä¢ Thi·∫øt b·ªã reranker: {self.device}\")\n\n    # =========================================================\n    # Topic priority (placeholder, can extend)\n    # =========================================================\n    def _apply_topic_priority(self, query: str, text: str) -> float:\n        # Example: boost when query contains keywords; keep 1.0 by default\n        q = query.lower()\n        if \"m·∫•t nƒÉng l·ª±c\" in q or \"gi√°m h·ªô\" in q or \"giao d·ªãch\" in q:\n            # small boost for chunks that explicitly mention ƒêi·ªÅu/Kho·∫£n related\n            if re.search(r\"ƒêi·ªÅu\\s+\\d+\", text):\n                return 1.15\n        return 1.0\n\n    # =========================================================\n    # Cross-encoder rerank. Returns list of (text, score)\n    # =========================================================\n    def _rerank(self, query: str, candidates: List[Tuple[str, float]]) -> List[Tuple[str, float]]:\n        if not candidates:\n            return []\n\n        pairs = [(query, c[0]) for c in candidates]\n        scores = []\n\n        for batch_start in range(0, len(pairs), self.batch_size):\n            batch = pairs[batch_start: batch_start + self.batch_size]\n            # tokenizer accepts list of tuples for some HF tokenizers; for safety, join with special sep\n            # we use encoding of pairs as two sentences\n            enc = self.reranker_tokenizer(\n                [q for q, _ in batch],\n                [c for _, c in batch],\n                padding=True,\n                truncation=True,\n                return_tensors=\"pt\",\n                max_length=512,\n            )\n            enc = {k: v.to(self.device) for k, v in enc.items()}\n            with torch.no_grad():\n                logits = self.reranker_model(**enc).logits  # shape (batch, num_labels)\n                # If num_labels >1, pick first logit or compute score appropriately.\n                # Many cross-encoders return single logit per pair (regression); handle both cases:\n                if logits.dim() == 2 and logits.size(1) == 1:\n                    batch_scores = torch.sigmoid(logits).squeeze(-1).cpu().numpy()\n                else:\n                    # fallback: take softmax over labels and take max-prob\n                    probs = torch.softmax(logits, dim=1)\n                    batch_scores = probs[:, 0].cpu().numpy()  # heuristic\n            scores.extend(batch_scores.tolist())\n\n        reranked = [(c[0], float(s)) for c, s in zip(candidates, scores)]\n        reranked.sort(key=lambda x: x[1], reverse=True)\n        return reranked\n\n    # =========================================================\n    # Retrieve top_k candidates\n    # =========================================================\n    def retrieve(self, query: str, top_k: int = None) -> List[Tuple[str, float]]:\n        if self.faiss_index is None or self.bm25 is None:\n            raise RuntimeError(\"‚ùå Ch∆∞a c√≥ index. H√£y g·ªçi build_index() tr∆∞·ªõc khi truy v·∫•n.\")\n\n        if top_k is None:\n            top_k = self.top_k\n\n        # --- FAISS semantic search ---\n        q_emb = self.embedder.encode([query], convert_to_numpy=True)\n        q_emb = np.asarray(q_emb, dtype=np.float32)\n        faiss.normalize_L2(q_emb)\n        D, I = self.faiss_index.search(q_emb, top_k)\n        faiss_results = []\n        for idx, score in zip(I[0], D[0]):\n            if idx < 0:\n                continue\n            faiss_results.append((self.text_chunks[int(idx)], float(score)))\n\n        # --- BM25 lexical search ---\n        bm25_scores = self.bm25.get_scores(re.findall(r'\\w+', query.lower()))\n        bm25_top_idx = np.argsort(bm25_scores)[::-1][:top_k]\n        bm25_results = [(self.text_chunks[int(i)], float(bm25_scores[int(i)])) for i in bm25_top_idx]\n\n        # --- Merge, keep best score per doc ---\n        merged: Dict[str, float] = {}\n        for text, score in faiss_results + bm25_results:\n            merged[text] = max(merged.get(text, 0.0), float(score))\n\n        # --- Apply topic boosting ---\n        boosted = [(text, score * self._apply_topic_priority(query, text)) for text, score in merged.items()]\n\n        # --- Rerank by cross-encoder ---\n        reranked = self._rerank(query, boosted)\n\n        # --- Final top_k ---\n        results = reranked[:top_k]\n\n        # Print brief preview for debugging\n        print(f\"üîé Truy v·∫•n: {query!s}\")\n        print(f\"üìë Top {len(results)} k·∫øt qu·∫£:\")\n        for i, (text, score) in enumerate(results, 1):\n            preview = text.replace(\"\\n\", \" \")[:200]\n            print(f\"   {i:02d}. ({score:.4f}) {preview}...\")\n\n        return results\n\n    def __repr__(self):\n        n = len(self.text_chunks)\n        dim = self.embeddings.shape[1] if self.embeddings is not None else \"?\"\n        return f\"<HybridLegalRetriever: {n} docs | dim={dim} | device={self.device}>\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:58.782930Z","iopub.execute_input":"2025-12-17T10:41:58.783187Z","iopub.status.idle":"2025-12-17T10:41:58.799537Z","shell.execute_reply.started":"2025-12-17T10:41:58.783162Z","shell.execute_reply":"2025-12-17T10:41:58.799018Z"}},"outputs":[{"name":"stdout","text":"Overwriting LegalRetriever.py\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"%%writefile LegalRAGProcessor.py\nimport re\nimport ast\nimport textwrap\nimport torch\nfrom typing import List, Tuple\n\n\nLEGAL_PROMPT = \"\"\"\\\nB·∫°n l√† **tr·ª£ l√Ω ph√°p l√Ω chuy√™n nghi·ªáp**, chuy√™n t∆∞ v·∫•n v√† ph√¢n t√≠ch **theo ph√°p lu·∫≠t Vi·ªát Nam**.\n\n- Kh√¥ng tr√≠ch nguy√™n vƒÉn to√†n b·ªô ƒëi·ªÅu lu·∫≠t, ch·ªâ n√™u s·ªë ƒëi·ªÅu, kho·∫£n, ch∆∞∆°ng.\n\n- Kh√¥ng vi·∫øt lan man ho·∫∑c l·∫≠p lu·∫≠n ngo√†i quy ƒë·ªãnh ph√°p lu·∫≠t.\n\n\n\n‚öñÔ∏è H∆∞·ªõng d·∫´n ƒë·ªãnh d·∫°ng b·∫Øt bu·ªôc:\n\nTr·∫£ l·ªùi theo **ƒë√∫ng 4 m·ª•c ƒë√°nh s·ªë sau** (b·∫Øt bu·ªôc):\n\n1. **Lƒ©nh v·ª±c** ‚Äì x√°c ƒë·ªãnh lƒ©nh v·ª±c lu·∫≠t √°p d·ª•ng (d√¢n s·ª±, lao ƒë·ªông, h√¥n nh√¢n v√† gia ƒë√¨nh, h√¨nh s·ª±, ...).\n\n2. **CƒÉn c·ª© ph√°p l√Ω** ‚Äì ghi r√µ [CH∆Ø∆†NG], [ƒêI·ªÄU LU·∫¨T], [KHO·∫¢N] c√≥ li√™n quan.\n\n3. **Ph√¢n t√≠ch** ‚Äì gi·∫£i th√≠ch ng·∫Øn g·ªçn t√¨nh hu·ªëng theo quy ƒë·ªãnh ph√°p lu·∫≠t.\n\n4. **K·∫øt lu·∫≠n** ‚Äì n√™u r√µ giao d·ªãch, h√†nh vi, ho·∫∑c t√¨nh hu·ªëng c√≥ h·ª£p l·ªá hay kh√¥ng, v√† ƒëi·ªÅu ki·ªán c·∫ßn c√≥.\n\n\n\n---\n\n\n\n**C√¢u h·ªèi:**\n\n{query}\n\n\n\n**C√°c cƒÉn c·ª© ph√°p l√Ω ƒë∆∞·ª£c truy xu·∫•t t·ª´ kho d·ªØ li·ªáu (gi·ªØ nguy√™n k√Ω hi·ªáu):**\n\n{context_text}\n\n\n\n---\n\n\n\n‚û°Ô∏è **Ph·∫ßn tr·∫£ l·ªùi ch√≠nh th·ª©c:**\n\"\"\"\n\n\nclass LegalRAGProcessor:\n    def __init__(self, llm_adapter, retriever, top_k=5, max_new_tokens=512):\n        self.llm = llm_adapter\n        self.retriever = retriever\n        self.top_k = top_k\n        self.max_new_tokens = max_new_tokens\n\n    def _build_prompt(self, query, context_text):\n        return LEGAL_PROMPT.format(query=query.strip(), context_text=context_text.strip())\n\n    def _clean_context(self, docs: List[Tuple[str, float]]) -> str:\n        merged = []\n        seen = set()\n        for i, (text, score) in enumerate(docs, 1):\n            cleaned = re.sub(r'[ \\t]+', ' ', text.strip())\n            cleaned = cleaned.replace(\"√è\", \"I\")\n            if cleaned not in seen:\n                seen.add(cleaned)\n                merged.append(f\"[ƒêO·∫†N {i}] {cleaned}\")\n        return \"\\n\\n\".join(merged[: self.top_k])\n\n    def _generate(self, prompt_text: str) -> str:\n        inputs = self.llm.tokenizer(prompt_text, return_tensors=\"pt\", truncation=True).to(self.llm.device)\n        outputs = self.llm.model.generate(\n            **inputs,\n            max_new_tokens=self.max_new_tokens,\n            temperature=0.3,\n            top_p=0.9,\n            repetition_penalty=1.1,\n            do_sample=False,\n            eos_token_id=self.llm.tokenizer.eos_token_id,\n        )\n        text = self.llm.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return self._clean_response(text)\n\n    def _try_parse_dict_text(self, text: str):\n        m = re.search(r\"\\{.*\\}\", text, re.S)\n        if not m:\n            return None\n        blob = m.group(0)\n        try:\n            parsed = ast.literal_eval(blob)\n            if isinstance(parsed, dict):\n                return parsed\n        except Exception:\n            try:\n                clean_blob = blob.replace(\"\\n\", \" \").replace(\"None\", \"null\")\n                parsed = ast.literal_eval(clean_blob)\n                if isinstance(parsed, dict):\n                    return parsed\n            except Exception:\n                return None\n        return None\n\n    def _extract_law_citations(self, text: str) -> str:\n        if not text:\n            return \"\"\n        t = re.sub(r\"\\s+\", \" \", text)\n        matches = []\n        for m in re.finditer(r\"ƒêi·ªÅu\\s*\\d+\", t, flags=re.I):\n            s = m.group(0)\n            tail = \"\"\n            rest = t[m.end(): m.end() + 60]\n            k = re.search(r\"(Kho·∫£n|kho·∫£n)\\s*\\d+\", rest)\n            if k:\n                tail = \" \" + k.group(0)\n            d = re.search(r\"(ƒêi·ªÉm|ƒëi·ªÉm)\\s*[a-z0-9]+\", rest)\n            if d:\n                tail += \" \" + d.group(0)\n            matches.append((s + tail).strip())\n        if not matches:\n            for m in re.finditer(r\"Kho·∫£n\\s*\\d+\", t, flags=re.I):\n                matches.append(m.group(0))\n        uniq = []\n        for it in matches:\n            it_norm = re.sub(r\"\\s+\", \" \", it).strip()\n            if it_norm not in uniq:\n                uniq.append(it_norm)\n        return \", \".join(uniq)\n\n    def _clean_cite_field(self, cite_text: str, context_text: str) -> str:\n        c = \"\"\n        if cite_text and len(cite_text) < 300:\n            c = self._extract_law_citations(cite_text)\n        if not c:\n            c = self._extract_law_citations(context_text)\n        if not c and context_text:\n            tags = re.findall(r\"\\[ƒêI·ªÄU LU·∫¨T\\]\\s*ƒêi·ªÅu\\s*\\d+\", context_text, flags=re.I)\n            tags += re.findall(r\"\\[KHO·∫¢N\\]\\s*Kho·∫£n\\s*\\d+\", context_text, flags=re.I)\n            tags = [re.sub(r\"\\[.*?\\]\\s*\", \"\", t).strip() for t in tags]\n            c = \", \".join(dict.fromkeys(tags))\n        return c or \"Ch∆∞a x√°c ƒë·ªãnh\"\n\n    def _clean_response(self, text: str) -> str:\n        text = re.sub(r\"(?is)^.*?Ph·∫ßn tr·∫£ l·ªùi ch√≠nh th·ª©c[:Ôºö]\\s*\", \"\", text).strip()\n        original_text = text\n        \n        structured = {\n            \"Lƒ©nh v·ª±c\": \"\",\n            \"CƒÉn c·ª© ph√°p l√Ω\": \"\",\n            \"Ph√¢n t√≠ch\": \"\",\n            \"K·∫øt lu·∫≠n\": \"\"\n        }\n\n        parsed = self._try_parse_dict_text(text)\n        if parsed and isinstance(parsed, dict):\n            for raw_key, value in parsed.items():\n                if not isinstance(raw_key, str):\n                    continue\n                key_lower = raw_key.strip().lower()\n                if any(x in key_lower for x in [\"lƒ©nh v·ª±c\", \"linh v·ª±c\", \"field\"]):\n                    structured[\"Lƒ©nh v·ª±c\"] = str(value).strip()\n                elif any(x in key_lower for x in [\"cƒÉn c·ª©\", \"can cu\", \"legal\", \"ph√°p l√Ω\"]):\n                    structured[\"CƒÉn c·ª© ph√°p l√Ω\"] = str(value).strip()\n                elif any(x in key_lower for x in [\"ph√¢n t√≠ch\", \"phan tich\", \"analysis\"]):\n                    structured[\"Ph√¢n t√≠ch\"] = str(value).strip()\n                elif any(x in key_lower for x in [\"k·∫øt lu·∫≠n\", \"ket luan\", \"conclusion\"]):\n                    structured[\"K·∫øt lu·∫≠n\"] = str(value).strip()\n        else:\n            lines = text.split('\\n')\n            current_section = None\n            section_content = []\n            for line in lines:\n                line_stripped = line.strip()\n                if not line_stripped:\n                    continue\n                is_heading = False\n                if re.search(r\"^\\s*1[\\.\\):]?\\s*\\*{0,2}Lƒ©nh\\s*v·ª±c\", line_stripped, flags=re.I):\n                    if current_section and section_content:\n                        structured[current_section] = \" \".join(section_content).strip()\n                    current_section = \"Lƒ©nh v·ª±c\"\n                    section_content = []\n                    content = re.sub(r\"^\\s*1[\\.\\):]?\\s*\\*{0,2}Lƒ©nh\\s*v·ª±c.*?:\\s*\", \"\", line_stripped, flags=re.I)\n                    if content:\n                        section_content.append(content)\n                    is_heading = True\n                elif re.search(r\"^\\s*2[\\.\\):]?\\s*\\*{0,2}CƒÉn\\s*c·ª©\\s*ph√°p\\s*l√Ω\", line_stripped, flags=re.I):\n                    if current_section and section_content:\n                        structured[current_section] = \" \".join(section_content).strip()\n                    current_section = \"CƒÉn c·ª© ph√°p l√Ω\"\n                    section_content = []\n                    content = re.sub(r\"^\\s*2[\\.\\):]?\\s*\\*{0,2}CƒÉn\\s*c·ª©\\s*ph√°p\\s*l√Ω.*?:\\s*\", \"\", line_stripped, flags=re.I)\n                    if content:\n                        section_content.append(content)\n                    is_heading = True\n                elif re.search(r\"^\\s*3[\\.\\):]?\\s*\\*{0,2}Ph√¢n\\s*t√≠ch\", line_stripped, flags=re.I):\n                    if current_section and section_content:\n                        structured[current_section] = \" \".join(section_content).strip()\n                    current_section = \"Ph√¢n t√≠ch\"\n                    section_content = []\n                    content = re.sub(r\"^\\s*3[\\.\\):]?\\s*\\*{0,2}Ph√¢n\\s*t√≠ch.*?:\\s*\", \"\", line_stripped, flags=re.I)\n                    if content:\n                        section_content.append(content)\n                    is_heading = True\n                elif re.search(r\"^\\s*4[\\.\\):]?\\s*\\*{0,2}K·∫øt\\s*lu·∫≠n\", line_stripped, flags=re.I):\n                    if current_section and section_content:\n                        structured[current_section] = \" \".join(section_content).strip()\n                    current_section = \"K·∫øt lu·∫≠n\"\n                    section_content = []\n                    content = re.sub(r\"^\\s*4[\\.\\):]?\\s*\\*{0,2}K·∫øt\\s*lu·∫≠n.*?:\\s*\", \"\", line_stripped, flags=re.I)\n                    if content:\n                        section_content.append(content)\n                    is_heading = True\n                if not is_heading and current_section:\n                    if not (line_stripped.startswith(\"'\") or line_stripped.startswith(\"{\") or line_stripped.startswith(\"}\")):\n                        section_content.append(line_stripped)\n            if current_section and section_content:\n                structured[current_section] = \" \".join(section_content).strip()\n\n        if not structured[\"Lƒ©nh v·ª±c\"]:\n            linh_vuc_patterns = [\n                r\"(?:Lƒ©nh\\s*v·ª±c|Field)[:Ôºö\\-‚Äì]?\\s*([^\\n\\.\\,]{5,80})\",\n                r\"thu·ªôc\\s+lƒ©nh\\s+v·ª±c\\s+([^\\n\\.\\,]{5,50})\",\n                r\"(?:Lu·∫≠t|LU·∫¨T)\\s+([A-ZƒêƒÇ√Ç√ä√î∆†∆Ø][a-zƒëƒÉ√¢√™√¥∆°∆∞\\s]+)\",\n            ]\n            for pattern in linh_vuc_patterns:\n                match = re.search(pattern, original_text, flags=re.I)\n                if match:\n                    structured[\"Lƒ©nh v·ª±c\"] = match.group(1).strip()\n                    break\n\n        if not structured[\"CƒÉn c·ª© ph√°p l√Ω\"]:\n            structured[\"CƒÉn c·ª© ph√°p l√Ω\"] = self._extract_law_citations(original_text)\n\n        if not structured[\"Ph√¢n t√≠ch\"]:\n            phan_tich_patterns = [\n                r\"(?:Ph√¢n\\s*t√≠ch|Analysis)[:Ôºö\\-‚Äì]?\\s*([^\\n]{50,400})\",\n                r\"(?:Theo|cƒÉn c·ª©|d·ª±a v√†o).*?(?:ƒêi·ªÅu|Kho·∫£n).*?[,\\.]?\\s*([^\\n]{50,300})\",\n            ]\n            for pattern in phan_tich_patterns:\n                match = re.search(pattern, original_text, flags=re.I | re.S)\n                if match:\n                    structured[\"Ph√¢n t√≠ch\"] = match.group(1).strip()\n                    break\n\n        if not structured[\"K·∫øt lu·∫≠n\"]:\n            ket_luan_patterns = [\n                r\"(?:K·∫øt\\s*lu·∫≠n|Conclusion)[:Ôºö\\-‚Äì]?\\s*([^\\n]{20,400})\",\n                r\"(?:V·∫≠y|Do ƒë√≥|Nh∆∞ v·∫≠y)[,\\s]+([^\\n]{20,300})\",\n                r\"(?:h·ª£p\\s*l·ªá|kh√¥ng\\s*h·ª£p\\s*l·ªá|ƒë∆∞·ª£c\\s*ph√©p|kh√¥ng\\s*ƒë∆∞·ª£c\\s*ph√©p).*?([^\\n]{10,200})\",\n            ]\n            for pattern in ket_luan_patterns:\n                match = re.search(pattern, original_text, flags=re.I)\n                if match:\n                    structured[\"K·∫øt lu·∫≠n\"] = match.group(1).strip()\n                    break\n\n        structured[\"CƒÉn c·ª© ph√°p l√Ω\"] = self._clean_cite_field(structured.get(\"CƒÉn c·ª© ph√°p l√Ω\", \"\"), original_text)\n\n        for key in [\"Ph√¢n t√≠ch\", \"K·∫øt lu·∫≠n\", \"Lƒ©nh v·ª±c\"]:\n            value = structured.get(key, \"\") or \"\"\n            value = re.sub(r\"^[\\'\\\"\\{\\[].*?[\\'\\\"\\}\\]]\\s*[:,]?\\s*\", \"\", value)\n            value = re.sub(r\"[\\{\\}\\[\\]]+\", \"\", value)\n            value = re.sub(r\"[\\'\\\"]{2,}\", \"\", value)\n            value = re.sub(r\"\\s+\", \" \", value).strip()\n            if len(value) > 500:\n                value = textwrap.shorten(value, width=500, placeholder=\"...\")\n            structured[key] = value\n\n        # =========================\n        # CLEAN-UP K·∫æT LU·∫¨N CH·∫∂T H∆†N\n        #  - Lo·∫°i b·ªè m·ªçi m·∫£nh dict-like ho·∫∑c nh√£n tr∆∞·ªùng c√≤n s√≥t\n        #  - Lo·∫°i b·ªè c√°c fragment d·∫°ng: , 'Lƒ©nh v·ª±c': '...'\n        #  - Gi·ªØ t·ªëi ƒëa 1-2 c√¢u sau khi d·ªçn s·∫°ch\n        # =========================\n        if structured[\"K·∫øt lu·∫≠n\"]:\n            kl = structured[\"K·∫øt lu·∫≠n\"]\n\n            # 1) lo·∫°i b·ªè c√°c fragment dict-like: 'Lƒ©nh v·ª±c': '...'\n            kl = re.sub(r\"[,\\s]*['\\\"]?\\s*(Lƒ©nh v·ª±c|CƒÉn c·ª© ph√°p l√Ω|Ph√¢n t√≠ch|K·∫øt lu·∫≠n)\\s*['\\\"]?\\s*[:Ôºö]\\s*['\\\"]?[^,'\\\"\\}\\]]+['\\\"]?\", \"\", kl, flags=re.I)\n\n            # 2) lo·∫°i b·ªè c√°c fragment d·∫°ng , Lƒ©nh v·ª±c: ... ho·∫∑c , Lƒ©nh v·ª±c '...'\n            kl = re.sub(r\",\\s*(Lƒ©nh v·ª±c|CƒÉn c·ª© ph√°p l√Ω|Ph√¢n t√≠ch|K·∫øt lu·∫≠n)\\s*[:Ôºö]\\s*[^,\\.]+\", \"\", kl, flags=re.I)\n\n            # 3) lo·∫°i b·ªè b·∫•t k·ª≥ c·∫∑p kh√≥a-gi√° tr·ªã d·∫°ng key=... ho·∫∑c key:... c√≤n s√≥t\n            kl = re.sub(r\"\\b(Lƒ©nh v·ª±c|CƒÉn c·ª© ph√°p l√Ω|Ph√¢n t√≠ch|K·∫øt lu·∫≠n)\\b\\s*[=:]\\s*[^,\\.]{1,200}\", \"\", kl, flags=re.I)\n\n            # 4) d·ªçn s·∫°ch ngo·∫∑c, ngo·∫∑c nh·ªçn, d·∫•u nh√°y d∆∞\n            kl = re.sub(r\"[\\{\\}\\[\\]\\\"]+\", \"\", kl)\n            kl = re.sub(r\"\\s+\", \" \", kl).strip()\n\n            # 5) c·∫Øt ch·ªâ 1-2 c√¢u ch·ªët\n            sentences = re.split(r\"(?<=[.!?])\\s+\", kl)\n            # Remove any empty strings\n            sentences = [s.strip() for s in sentences if s.strip()]\n            if sentences:\n                kl_final = \" \".join(sentences[:2])\n            else:\n                kl_final = kl.strip()\n            structured[\"K·∫øt lu·∫≠n\"] = kl_final\n\n        defaults = {\n            \"Lƒ©nh v·ª±c\": \"Lu·∫≠t D√¢n s·ª±\",\n            \"CƒÉn c·ª© ph√°p l√Ω\": \"Ch∆∞a x√°c ƒë·ªãnh c·ª• th·ªÉ\",\n            \"Ph√¢n t√≠ch\": \"C·∫ßn xem x√©t c√°c quy ƒë·ªãnh ph√°p lu·∫≠t c√≥ li√™n quan ƒë·ªÉ ƒë∆∞a ra ph√¢n t√≠ch ch√≠nh x√°c.\",\n            \"K·∫øt lu·∫≠n\": \"C·∫ßn th√™m th√¥ng tin ƒë·ªÉ k·∫øt lu·∫≠n ch√≠nh x√°c v·ªÅ t√≠nh h·ª£p l·ªá.\"\n        }\n        for key in [\"Lƒ©nh v·ª±c\", \"CƒÉn c·ª© ph√°p l√Ω\", \"Ph√¢n t√≠ch\", \"K·∫øt lu·∫≠n\"]:\n            if not structured[key] or len(structured[key]) < 3:\n                structured[key] = defaults[key]\n\n        final = []\n        order = [\"Lƒ©nh v·ª±c\", \"CƒÉn c·ª© ph√°p l√Ω\", \"Ph√¢n t√≠ch\", \"K·∫øt lu·∫≠n\"]\n        for i, key in enumerate(order, start=1):\n            content = structured[key].strip()\n            final.append(f\"{i}. **{key}** ‚Äì {content}\")\n        return \"\\n\\n\".join(final)\n\n    def _validate_structure(self, text: str) -> bool:\n        required = [\"Lƒ©nh v·ª±c\", \"CƒÉn c·ª© ph√°p l√Ω\", \"Ph√¢n t√≠ch\", \"K·∫øt lu·∫≠n\"]\n        return all(k in text for k in required)\n\n    def _regenerate_if_incomplete(self, text: str, query: str, context_text: str) -> str:\n        if self._validate_structure(text):\n            return text\n        repair_prompt = (\n            f\"C√¢u tr·∫£ l·ªùi tr√™n ch∆∞a ƒë√∫ng 4 m·ª•c. H√£y vi·∫øt l·∫°i ng·∫Øn g·ªçn theo 4 m·ª•c:\\n\"\n            f\"1. **Lƒ©nh v·ª±c** ‚Äì lƒ©nh v·ª±c lu·∫≠t √°p d·ª•ng\\n\"\n            f\"2. **CƒÉn c·ª© ph√°p l√Ω** ‚Äì ch·ªâ n√™u [CH∆Ø∆†NG], [ƒêI·ªÄU LU·∫¨T], [KHO·∫¢N]\\n\"\n            f\"3. **Ph√¢n t√≠ch** ‚Äì gi·∫£i th√≠ch ng·∫Øn g·ªçn\\n\"\n            f\"4. **K·∫øt lu·∫≠n** ‚Äì n√™u r√µ t√≠nh h·ª£p l·ªá v√† ƒëi·ªÅu ki·ªán\\n\\n\"\n            f\"C√¢u h·ªèi: {query}\\n\\nCƒÉn c·ª© ph√°p l√Ω:\\n{context_text}\"\n        )\n        inputs = self.llm.tokenizer(repair_prompt, return_tensors=\"pt\", truncation=True).to(self.llm.device)\n        outputs = self.llm.model.generate(\n            **inputs, \n            max_new_tokens=self.max_new_tokens, \n            temperature=0.3\n        )\n        fixed = self.llm.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return self._clean_response(fixed)\n\n    def process_query(self, query: str, top_k: int = None) -> str:\n        docs = self.retriever.retrieve(query, top_k or self.top_k)\n        context_text = self._clean_context(docs)\n        prompt = self._build_prompt(query, context_text)\n        answer = self._generate(prompt)\n        return self._regenerate_if_incomplete(answer, query, context_text).strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:58.800524Z","iopub.execute_input":"2025-12-17T10:41:58.800798Z","iopub.status.idle":"2025-12-17T10:41:58.822735Z","shell.execute_reply.started":"2025-12-17T10:41:58.800777Z","shell.execute_reply":"2025-12-17T10:41:58.822156Z"}},"outputs":[{"name":"stdout","text":"Overwriting LegalRAGProcessor.py\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%writefile build_rag_from_txt_folder.py\nimport re\nimport os\nfrom tqdm import tqdm\nfrom typing import List\n\ndef normalize_legal_text(text: str) -> str:\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r\"(?<=ƒêi·ªÅu)\\s+(\\d+)\", r\" \\1.\", text)\n    text = re.sub(r\"(?<=Kho·∫£n)\\s+(\\d+)\", r\" \\1.\", text)\n    text = re.sub(r\"(?<!\\.)\\s*(?=ƒêi·ªÅu\\s+\\d+\\.)\", \"\\n\", text)\n    text = re.sub(r\"(?<!\\.)\\s*(?=Kho·∫£n\\s+\\d+\\.)\", \"\\n\", text)\n    return text.strip()\n\ndef _tag_structures(text: str) -> str:\n    \"\"\"G·∫Øn tag [CH∆Ø∆†NG], [ƒêI·ªÄU], [KHO·∫¢N]\"\"\"\n    chapter = re.findall(r\"(CH∆Ø∆†NG\\s+[IVXLC]+[^ƒê]*)\", text)\n    article = re.findall(r\"(ƒêI·ªÄU LU·∫¨T\\s+\\d+)\", text)\n    clause = re.findall(r\"(KHO·∫¢N\\s+\\d+)\", text)\n\n    ch = f\"[CH∆Ø∆†NG] {chapter[0].strip()}\" if chapter else \"\"\n    di = f\"[ƒêI·ªÄU LU·∫¨T] {article[0].strip()}\" if article else \"\"\n    kh = f\"[KHO·∫¢N] {clause[0].strip()}\" if clause else \"\"\n\n    return \" \".join([ch, di, kh]).strip()\n\ndef split_legal_text(text: str, max_length: int = 1000) -> List[str]:\n    \"\"\"T√°ch vƒÉn b·∫£n lu·∫≠t v√† g·∫Øn nh√£n c·∫•u tr√∫c.\"\"\"\n    text = re.sub(r\"\\s+\", \" \", text)\n    sections = re.split(r\"(?=ƒêi·ªÅu\\s+\\d+\\.?)\", text)\n    refined_chunks = []\n\n    for sec in sections:\n        if len(sec.strip()) < 20:\n            continue\n        if len(sec) > max_length * 2:\n            subchunks = re.split(r\"(?=Kho·∫£n\\s+\\d+\\.?)\", sec)\n            refined_chunks += [f\"{_tag_structures(s)} {s.strip()}\" for s in subchunks if len(s.strip()) > 50]\n        else:\n            refined_chunks.append(f\"{_tag_structures(sec)} {sec.strip()}\")\n\n    return [c.strip() for c in refined_chunks if len(c.strip()) > 50]\n\n\n\ndef extract_text_from_txt(txt_path: str) -> str:\n    \"\"\"ƒê·ªçc n·ªôi dung vƒÉn b·∫£n t·ª´ file .txt (UTF-8).\"\"\"\n    try:\n        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n        return text\n    except UnicodeDecodeError:\n        # fallback n·∫øu file m√£ h√≥a kh√°c\n        with open(txt_path, \"r\", encoding=\"utf-8-sig\", errors=\"ignore\") as f:\n            return f.read()\n\n\ndef build_rag_from_txt_folder(folder_path: str):\n    \"\"\"\n    ƒê·ªçc t·∫•t c·∫£ file .txt trong th∆∞ m·ª•c, chia nh·ªè th√†nh c√°c ƒëo·∫°n (chunks)\n    ƒë·ªÉ d√πng l√†m d·ªØ li·ªáu cho m√¥ h√¨nh RAG.\n    \"\"\"\n    txt_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".txt\")]\n    print(f\"üìö ƒêang ƒë·ªçc {len(txt_files)} t·ªáp TXT t·ª´: {folder_path}\")\n\n    all_chunks = []\n    for file_name in tqdm(txt_files):\n        txt_path = os.path.join(folder_path, file_name)\n        try:\n            text = extract_text_from_txt(txt_path)\n            # ‚úÖ D√πng h√†m m·ªõi\n            chunks = split_legal_text(text, max_length=1000)\n            all_chunks.extend(chunks)\n        except Exception as e:\n            print(f\"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω {file_name}: {e}\")\n\n    print(f\"‚úÖ T·ªïng s·ªë ƒëo·∫°n vƒÉn b·∫£n (chunks): {len(all_chunks)}\")\n    return all_chunks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:58.825039Z","iopub.execute_input":"2025-12-17T10:41:58.825242Z","iopub.status.idle":"2025-12-17T10:41:58.842262Z","shell.execute_reply.started":"2025-12-17T10:41:58.825227Z","shell.execute_reply":"2025-12-17T10:41:58.841533Z"}},"outputs":[{"name":"stdout","text":"Writing build_rag_from_txt_folder.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"%%writefile Decoder.py\nimport gc, torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass Decoder:\n    \"\"\"\n    Decoder v·ªõi prompt template ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ sinh ƒë·∫ßy ƒë·ªß 4 ph·∫ßn\n    \"\"\"\n    def __init__(self, model_id):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True,use_fast=False)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16,\n            #device_map = 'auto'\n            low_cpu_mem_usage=True\n        ).eval()\n        torch.set_grad_enabled(False)\n    \n    def _build_context(self, question, retrieved_context):\n        \"\"\"\n        Prompt template ƒë∆∞·ª£c c·∫£i thi·ªán ƒë·ªÉ ƒë·∫£m b·∫£o model sinh ƒë·ªß 4 ph·∫ßn\n        \"\"\"\n        return f\"\"\"B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω chuy√™n nghi·ªáp c·ªßa Vi·ªát Nam. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi ph√°p l√Ω theo ƒë·ªãnh d·∫°ng CHU·∫®N 4 ph·∫ßn b√™n d∆∞·ªõi.\n\nüìö CƒÇN C·ª® PH√ÅP LU·∫¨T:\n{retrieved_context}\n\n‚ùì C√ÇU H·ªéI: {question}\n\n‚ö†Ô∏è Y√äU C·∫¶U B·∫ÆT BU·ªòC - Tr·∫£ l·ªùi theo ƒê√öNG 4 PH·∫¶N sau (kh√¥ng ƒë∆∞·ª£c b·ªè s√≥t):\n\n1. **Lƒ©nh v·ª±c** ‚Äì X√°c ƒë·ªãnh r√µ lƒ©nh v·ª±c lu·∫≠t (V√≠ d·ª•: Lu·∫≠t D√¢n s·ª±, Lu·∫≠t H√¨nh s·ª±, Lu·∫≠t Lao ƒë·ªông...).\n\n2. **CƒÉn c·ª© ph√°p l√Ω** ‚Äì Ghi CH√çNH X√ÅC c√°c ƒêi·ªÅu, Kho·∫£n, Ch∆∞∆°ng li√™n quan (V√≠ d·ª•: ƒêi·ªÅu 123 B·ªô lu·∫≠t D√¢n s·ª± 2015).\n\n3. **Ph√¢n t√≠ch** ‚Äì Gi·∫£i th√≠ch CHI TI·∫æT t√¨nh hu·ªëng theo quy ƒë·ªãnh ph√°p lu·∫≠t (t·ªëi thi·ªÉu 2-3 c√¢u).\n\n4. **K·∫øt lu·∫≠n** ‚Äì Kh·∫≥ng ƒë·ªãnh R√ï R√ÄNG t√¨nh hu·ªëng c√≥ h·ª£p ph√°p/b·∫•t h·ª£p ph√°p hay kh√¥ng, v√† ƒëi·ªÅu ki·ªán c·∫ßn c√≥.\n\nQUAN TR·ªåNG: B·∫°n PH·∫¢I sinh ƒë·ªß c·∫£ 4 ph·∫ßn theo th·ª© t·ª± tr√™n. Kh√¥ng ƒë∆∞·ª£c b·ªè qua b·∫•t k·ª≥ ph·∫ßn n√†o.\n\"\"\"\n    def _generate_one(self, inputs, temperature, max_new_tokens):\n        with torch.inference_mode():\n            output = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id,\n                repetition_penalty=1.1  # Tr√°nh l·∫∑p l·∫°i\n            )\n        gen_ids = output[0][inputs[\"input_ids\"].shape[-1]:]\n        text = self.tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n        del output\n        torch.cuda.empty_cache()\n        gc.collect()\n        return text\n    \n    def decode(self, prompt, temperature=0.5, max_new_tokens=512, num_samples=1):\n        \"\"\"\n        Sinh tu·∫ßn t·ª± ƒë·ªÉ tr√°nh OOM, m·ªói l·∫ßn 1 c√¢u\n        \"\"\"\n        results = []\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        \n        for i in range(num_samples):\n            text = self._generate_one(inputs, temperature, max_new_tokens)\n            print(f\"\\nüß† H∆∞·ªõng #{i+1} (ƒë·ªô d√†i: {len(text)} k√Ω t·ª±)\")\n            print(text[:500], \"...\" if len(text) > 500 else \"\")\n            results.append(text)\n            torch.cuda.empty_cache()\n        \n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:58.842873Z","iopub.execute_input":"2025-12-17T10:41:58.843085Z","iopub.status.idle":"2025-12-17T10:41:58.860786Z","shell.execute_reply.started":"2025-12-17T10:41:58.843070Z","shell.execute_reply":"2025-12-17T10:41:58.860199Z"}},"outputs":[{"name":"stdout","text":"Writing Decoder.py\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"%%writefile legal_self_consistency_rag.py\nimport gc\nimport re\nimport time\nimport json\nimport torch\nfrom collections import Counter\nfrom sentence_transformers import SentenceTransformer, util\n\n# === GLOBAL: Shared Embedder ===\nEMBEDDER = SentenceTransformer(\"./vietnamese-bi-encoder\", device=\"cuda\")\n_EMB_BATCH_SIZE = 64\n_RETRIEVAL_CACHE = {}\n_SENT_EMB_CACHE = {}\n\n\ndef _embed_sentences_cached(sents, device=\"cuda\", batch_size=_EMB_BATCH_SIZE):\n    key = tuple(sents)\n    if key in _SENT_EMB_CACHE:\n        return _SENT_EMB_CACHE[key]\n    use_cpu = len(sents) > 200\n    enc_device = \"cpu\" if use_cpu else device\n    with torch.no_grad():\n        embs = EMBEDDER.encode(\n            sents,\n            convert_to_tensor=True,\n            device=enc_device,\n            show_progress_bar=False,\n            batch_size=batch_size\n        )\n    if enc_device == \"cpu\" and device == \"cuda\":\n        embs = embs.to(\"cuda\")\n    _SENT_EMB_CACHE[key] = embs\n    return embs\n\n\ndef _clean_model_output(text: str) -> str:\n    \"\"\"\n    Lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng c·∫ßn thi·∫øt t·ª´ output c·ªßa model\n    \"\"\"\n    if not text:\n        return \"\"\n\n    # 1. Lo·∫°i b·ªè \"C·∫£m ∆°n b·∫°n ƒë√£ tu√¢n th·ªß!\"\n    text = re.sub(r'(?i)c·∫£m\\s+∆°n.*?tu√¢n\\s+th·ªß.*?(?:\\n|\\r)+', '', text)\n\n    # 2. Lo·∫°i b·ªè ph·∫ßn ### Instruction v√† ### Response\n    text = re.sub(r'###\\s*Instruction:.*?(?=###\\s*Response:|$)', '', text, flags=re.DOTALL | re.IGNORECASE)\n    text = re.sub(r'###\\s*Response:\\s*', '', text, flags=re.IGNORECASE)\n\n    # 3. Lo·∫°i b·ªè emoji v√† c√°c k√Ω t·ª± kh√¥ng mong mu·ªën\n    text = re.sub(r'[\\U0001F300-\\U0001F6FF\\U0001F900-\\U0001F9FFüß†üôèüòä‚≠ê‚ú®üåüüí¨üìùüîÑ]+', '', text)\n\n    # 4. B·ªè ph·∫ßn \"C·∫£m ∆°n\" ·ªü cu·ªëi ho·∫∑c gi·ªØa\n    text = re.sub(r'(?i)^\\s*c·∫£m\\s+∆°n[\\s\\S]*$', '', text, flags=re.MULTILINE)\n\n    # 5. Trim v√† chu·∫©n ho√° kho·∫£ng tr·∫Øng\n    text = re.sub(r'\\r', '\\n', text)\n    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n    text = re.sub(r'[ \\t]+', ' ', text)\n\n    return text.strip()\n\ndef _extract_from_dict_format(text: str) -> dict:\n    \"\"\"\n    Tr√≠ch d·∫°ng dict t·ª´ output c·ªßa model.\n    ∆Øu ti√™n dict c√≥ ƒë·ªß 2‚Äì4 m·ª•c h·ª£p l·ªá. Kh√¥ng crash, kh√¥ng m·∫•t d·ªØ li·ªáu.\n    \"\"\"\n    sections = {}\n    if not text:\n        return sections\n\n    # T√¨m t·∫•t c·∫£ block {...}\n    dict_matches = re.findall(r'\\{[^{}]{10,2000}\\}',text)\n\n    if not dict_matches:\n        return {}\n\n    best = None\n    best_score = -1\n\n    # Ch·ªçn dict c√≥ nhi·ªÅu key h·ª£p l·ªá nh·∫•t\n    for d in dict_matches:\n        test = d.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"').replace(\"'\", '\"')\n        try:\n            parsed = json.loads(test)\n            if not isinstance(parsed, dict):\n               continue\n\n            valid_keys = [\"lƒ©nh\", \"linh\", \"cƒÉn\", \"can\", \"ph√¢n\", \"phan\", \"k·∫øt\", \"ket\"]\n            if len(parsed) > 6:\n               continue\n            if not any(any(vk in k.lower() for vk in valid_keys) for k in parsed.keys()):\n               continue\n            keys = parsed.keys()\n            score = sum(k in keys for k in ['Lƒ©nh v·ª±c', 'CƒÉn c·ª© ph√°p l√Ω', 'Ph√¢n t√≠ch', 'K·∫øt lu·∫≠n'])\n            if score > best_score:\n                best_score = score\n                best = parsed\n        except Exception:\n            continue\n\n    if not best:\n        return {}\n\n    parsed = best\n\n    # H√†m l·∫•y gi√° tr·ªã theo nhi·ªÅu t√™n kh·∫£ dƒ©\n    def take(*keys):\n        for k, v in parsed.items():\n            if any(x.lower() in k.lower() for x in keys):\n                return str(v).strip()\n        return \"\"\n\n    val = take(\"lƒ©nh\", \"linh\", \"field\", \"area\")\n    if isinstance(val, dict):\n        val = \"\"\n    sections = {\n    \"linh_vuc\": val,\n    \"can_cu\": take(\"cƒÉn\", \"can\", \"legal\", \"basis\"),\n    \"phan_tich\": take(\"ph√¢n\", \"phan\", \"analysis\", \"reasoning\"),\n    \"ket_luan\": take(\"k·∫øt\", \"ket\", \"conclusion\", \"result\")\n}\n\n\n    # Ch·ªâ tr·∫£ c√°c key c√≥ n·ªôi dung th·∫≠t s·ª±\n    sections = {k: v for k, v in sections.items() if v and len(v) > 3}\n\n    return sections\n\n    \ndef _parse_four_sections(text: str) -> dict:\n    \"\"\"\n    Parse vƒÉn b·∫£n th√†nh 4 ph·∫ßn b·∫Øt bu·ªôc.\n    Tr·∫£ v·ªÅ dict v·ªõi keys: linh_vuc, can_cu, phan_tich, ket_luan\n    \"\"\"\n    text = _clean_model_output(text)\n    sections = {\"linh_vuc\": \"\", \"can_cu\": \"\", \"phan_tich\": \"\", \"ket_luan\": \"\"}\n\n    # ===== FIX: t√°ch block dict JSON-like tr∆∞·ªõc khi regex =====\n    dict_sections = _extract_from_dict_format(text)\n    if dict_sections:\n        return dict_sections\n\n    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c dict ‚Üí khi ƒë√≥ m·ªõi lo·∫°i b·ªè block dict ƒë·ªÉ regex kh√¥ng ƒÉn nh·∫ßm\n    m_dict = re.search(r'\\{[\\s\\S]{10,5000}\\}', text)\n    if m_dict:\n        text = text.replace(m_dict.group(0), \"\").strip()\n\n    patterns = {\n    \"linh_vuc\": r\"(?:^|\\n)\\s*Lƒ©nh v·ª±c\\s*[:\\-‚Äì]\\s*(.+?)(?=\\n\\S)\",\n    \"can_cu\": r\"(?:^|\\n)\\s*CƒÉn c·ª© ph√°p l√Ω\\s*[:\\-‚Äì]\\s*(.+?)(?=\\n\\S)\",\n    \"phan_tich\": r\"(?:^|\\n)\\s*Ph√¢n t√≠ch\\s*[:\\-‚Äì]\\s*(.+?)(?=\\n\\S)\",\n    \"ket_luan\": r\"(?:^|\\n)\\s*K·∫øt lu·∫≠n\\s*[:\\-‚Äì]\\s*(.+)$\"\n    }\n    for key, patt in patterns.items():\n        m = re.search(patt, text, re.DOTALL)\n        if m:\n           sections[key] = m.group(1).strip()\n\n    # N·∫øu v·∫´n ch∆∞a c√≥, th·ª≠ t√°ch theo ƒëo·∫°n vƒÉn (double newline) v√† map heuristically\n    if not any(sections.values()):\n        parts = [p.strip() for p in re.split(r'\\n\\n+', text) if p.strip()]\n        if parts:\n            # heuristic mapping: ƒë·∫ßu ti√™n -> lƒ©nh v·ª±c, ti·∫øp -> cƒÉn c·ª©, ti·∫øp -> ph√¢n t√≠ch, cu·ªëi -> k·∫øt lu·∫≠n\n            order = ['linh_vuc', 'can_cu', 'phan_tich', 'ket_luan']\n            for i, part in enumerate(parts[:4]):\n                sections[order[i]] = part\n\n    return sections\n\n\ndef _deduplicate_sentences(text: str, threshold: float = 0.82) -> str:\n    \"\"\"\n    Lo·∫°i c√¢u l·∫∑p d·ª±a tr√™n cosine embedding trong t·ª´ng ph·∫ßn.\n    Gi·ªØ l·∫°i c√¢u mang th√¥ng tin m·ªõi.\n    \"\"\"\n    if not text:\n        return \"\"\n\n    text = text.strip()\n    parts = re.split(r'\\n\\s*\\n', text)\n    cleaned_parts = []\n\n    for part in parts:\n        sents = re.split(r'(?<=[.?!])\\s+', part)\n        sents = [s.strip() for s in sents if s.strip()]\n        if len(sents) <= 1:\n            cleaned_parts.append(part.strip())\n            continue\n\n        try:\n            embs = _embed_sentences_cached(sents)\n        except:\n            cleaned_parts.append(part.strip())\n            continue\n\n        kept = [sents[0]]\n        kept_embs = [embs[0]]\n\n        for i in range(1, len(sents)):\n            score = util.cos_sim(embs[i], torch.stack(kept_embs)).max().item()\n            if score < threshold:\n                kept.append(sents[i])\n                kept_embs.append(embs[i])\n\n        cleaned_parts.append(\" \".join(kept).strip())\n\n    return \"\\n\\n\".join(cleaned_parts).strip()\n\n\n\ndef extract_legal_conclusion(text: str):\n    \"\"\"Tr√≠ch xu·∫•t k·∫øt lu·∫≠n ph√°p l√Ω t·ª´ vƒÉn b·∫£n\"\"\"\n    if not text:\n        return \"\"\n    m = re.search(\n        r'([^.?!]*?(ƒêi·ªÅu\\s*\\d+|Kho·∫£n\\s*\\d+|Quy·∫øt\\s*ƒë·ªãnh|K·∫øt\\s*lu·∫≠n)[^.?!]*[.?!])',\n        text, re.I\n    )\n    if m:\n        return m.group(1).strip()\n    sents = re.split(r'(?<=[.?!])\\s+', text.strip())\n    for s in reversed(sents[-3:]):\n        if len(s.strip()) > 10:\n            return s.strip()\n    return text.strip()\n\n\ndef _validate_and_complete_sections(sections: dict, original_text: str) -> dict:\n    \"\"\"\n    Ki·ªÉm tra v√† b·ªï sung c√°c ph·∫ßn c√≤n thi·∫øu.\n    C·∫£i ti·∫øn: ph√°t hi·ªán lƒ©nh v·ª±c ph√°p l√Ω linh ho·∫°t, tr√°nh m·∫∑c ƒë·ªãnh 'D√¢n s·ª±' cho m·ªçi tr∆∞·ªùng h·ª£p.\n    \"\"\"\n    cleaned_original = _clean_model_output(original_text)\n    lower_text = cleaned_original.lower()\n\n    # ƒê·∫£m b·∫£o ƒë·ªß kh√≥a\n    for k in ['linh_vuc', 'can_cu', 'phan_tich', 'ket_luan']:\n        sections.setdefault(k, \"\")\n\n    # === üîπ Ph√°t hi·ªán lƒ©nh v·ª±c ph√°p l√Ω ===\n    if not sections['linh_vuc']:\n        field_map = {\n            \"d√¢n s·ª±\": [\n                \"h·ª£p ƒë·ªìng\", \"b·ªìi th∆∞·ªùng\", \"di ch√∫c\", \"th·ª´a k·∫ø\", \"quy·ªÅn s·ªü h·ªØu\", \"t√†i s·∫£n\", \"giao d·ªãch d√¢n s·ª±\"\n            ],\n            \"h√¥n nh√¢n v√† gia ƒë√¨nh\": [\n                \"ly h√¥n\", \"k·∫øt h√¥n\", \"nu√¥i con\", \"t√†i s·∫£n chung\", \"t√†i s·∫£n ri√™ng\", \"h√¥n nh√¢n\", \"gia ƒë√¨nh\", \"cha m·∫π\", \"con c√°i\"\n            ],\n            \"lao ƒë·ªông\": [\n                \"ng∆∞·ªùi lao ƒë·ªông\", \"h·ª£p ƒë·ªìng lao ƒë·ªông\", \"ti·ªÅn l∆∞∆°ng\", \"ngh·ªâ vi·ªác\", \"sa th·∫£i\", \"b·∫£o hi·ªÉm x√£ h·ªôi\", \"l√†m vi·ªác\", \"tr·ª£ c·∫•p\"\n            ]\n        }\n\n        scores = Counter()\n        for field, keywords in field_map.items():\n            for kw in keywords:\n                if re.search(rf\"\\b{re.escape(kw)}\\b\", lower_text):\n                    scores[field] += 1\n\n        if scores:\n            best_field = scores.most_common(1)[0][0]\n            sections['linh_vuc'] = f\"Lu·∫≠t {best_field.title()}\"\n        else:\n            sections['linh_vuc'] = \"Ch∆∞a x√°c ƒë·ªãnh r√µ ‚Äì c·∫ßn cƒÉn c·ª© th√™m v√†o t√¨nh ti·∫øt\"\n\n    # === üîπ B·ªï sung cƒÉn c·ª© ph√°p l√Ω ===\n    if not sections['can_cu']:\n        legal_refs = re.findall(r'(ƒêi·ªÅu\\s+\\d+[a-zA-Z]?|Kho·∫£n\\s+\\d+|Ch∆∞∆°ng\\s+[IVXLC]+)', cleaned_original, re.IGNORECASE)\n        if legal_refs:\n            seen = set()\n            unique_refs = []\n            for ref in legal_refs:\n                if ref not in seen:\n                    seen.add(ref)\n                    unique_refs.append(ref)\n            sections['can_cu'] = \", \".join(unique_refs[:5])\n        else:\n            sections['can_cu'] = \"Theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t hi·ªán h√†nh\"\n\n    # === üîπ B·ªï sung ph·∫ßn ph√¢n t√≠ch ===\n    if not sections['phan_tich']:\n        sentences = re.split(r'(?<=[.?!])\\s+', cleaned_original.strip())\n        meaningful = [s for s in sentences if len(s) > 30 and not s.strip().startswith(('1.', '2.', '3.', '4.', '###'))]\n        if meaningful:\n            sections['phan_tich'] = \" \".join(meaningful[:3])\n        else:\n            sections['phan_tich'] = \"Theo quy ƒë·ªãnh ph√°p lu·∫≠t, c·∫ßn xem x√©t c√°c y·∫øu t·ªë li√™n quan.\"\n\n    # === üîπ B·ªï sung ph·∫ßn k·∫øt lu·∫≠n ===\n    if not sections['ket_luan'] or len(sections['ket_luan']) < 15:\n        concl = extract_legal_conclusion(cleaned_original)\n        if concl:\n            sections['ket_luan'] = concl\n        else:\n            sections['ket_luan'] = \"C·∫ßn xem x√©t c·ª• th·ªÉ t·ª´ng tr∆∞·ªùng h·ª£p theo quy ƒë·ªãnh ph√°p lu·∫≠t.\"\n\n    # === üîπ Chu·∫©n ho√° l·∫°i c√°c ph·∫ßn ===\n    for key in sections:\n        if sections[key]:\n            sections[key] = re.sub(r'^[\\-‚Äì‚Äî\\s]+', '', sections[key]).strip()\n\n    return sections\n\n\ndef _legal_style_cleanup(text: str) -> str:\n    \"\"\"\n    Chu·∫©n h√≥a vƒÉn phong ph√°p l√Ω: kh√°ch quan, ch√≠nh x√°c, b·ªè c·∫£m t√≠nh.\n    \"\"\"\n    if not text:\n        return text\n\n    text = re.sub(r'\\b(r√µ r√†ng|hi·ªÉn nhi√™n|theo quan ƒëi·ªÉm c·ªßa t√¥i|ch√∫ng ta th·∫•y r·∫±ng|t√¥i cho r·∫±ng)\\b', '', text, flags=re.IGNORECASE)\n    text = re.sub(r'\\s{2,}', ' ', text)\n    text = re.sub(r'ƒëi·ªÅu\\s+(\\d+)', lambda m: f\"ƒêi·ªÅu {m.group(1)}\", text, flags=re.IGNORECASE)\n    text = text.replace(\" - \", \" ‚Äì \")\n    text = text.strip()\n    return text\n\n\ndef _format_legal_answer_enhanced(answer: str) -> str:\n    \"\"\"\n    Chu·∫©n ho√° c√¢u tr·∫£ l·ªùi th√†nh 4 ph·∫ßn: Lƒ©nh v·ª±c, CƒÉn c·ª© ph√°p l√Ω, Ph√¢n t√≠ch, K·∫øt lu·∫≠n\n    ƒê√É FIX l·ªói tr·ªôn nh·∫ßm CƒÉn c·ª© v√†o Lƒ©nh v·ª±c v√† tr√πng ƒë√°nh s·ªë.\n    \"\"\"\n    text = _clean_model_output(answer)\n    sections = _parse_four_sections(text)\n    sections = _validate_and_complete_sections(sections, text)\n    if all(sections[k] for k in ['linh_vuc', 'can_cu', 'phan_tich', 'ket_luan']):\n        return (\n        f\"1. **Lƒ©nh v·ª±c** ‚Äì {sections['linh_vuc']}\\n\\n\"\n        f\"2. **CƒÉn c·ª© ph√°p l√Ω** ‚Äì {sections['can_cu']}\\n\\n\"\n        f\"3. **Ph√¢n t√≠ch** ‚Äì {sections['phan_tich']}\\n\\n\"\n        f\"4. **K·∫øt lu·∫≠n** ‚Äì {sections['ket_luan']}\"\n    ).strip()\n    # Lo·∫°i b·ªè numbering/bold d∆∞ ·ªü ƒë·∫ßu ph·∫ßn\n    for key in sections:\n        if sections[key]:\n            sections[key] = re.sub(\n                r'^\\s*\\d+\\s*[.)\\-‚Äì]*\\s*', '',  # x√≥a \"1. \", \"1)\", \"1 -\", ...\n                sections[key], flags=re.IGNORECASE).strip()\n            sections[key] = re.sub(\n                r'^\\*\\*.+?\\*\\*\\s*[‚Äì:\\-]\\s*', '',\n                sections[key], flags=re.IGNORECASE).strip()\n\n    # D·ªçn vƒÉn phong\n    for k in sections:\n        sections[k] = _legal_style_cleanup(_deduplicate_sentences(sections[k]))\n\n    # Construct final standardized format (Duy nh·∫•t 1 l·∫ßn numbering)\n    return (\n        f\"1. **Lƒ©nh v·ª±c** ‚Äì {sections['linh_vuc']}\\n\\n\"\n        f\"2. **CƒÉn c·ª© ph√°p l√Ω** ‚Äì {sections['can_cu']}\\n\\n\"\n        f\"3. **Ph√¢n t√≠ch** ‚Äì {sections['phan_tich']}\\n\\n\"\n        f\"4. **K·∫øt lu·∫≠n** ‚Äì {sections['ket_luan']}\"\n    ).strip()\n\n\n# === Main RAG orchestration (example simplified) ===\n# Note: The functions below assume `retriever` and `decoder` objects are provided\n\ndef _retrieve_with_cache(retriever, query, top_k=5, use_cache=True):\n    key = (query, top_k)\n    if use_cache and key in _RETRIEVAL_CACHE:\n        return _RETRIEVAL_CACHE[key]\n    retrieved = retriever.retrieve(query, top_k=top_k)\n    normalized = []\n    for item in retrieved:\n        try:\n            if isinstance(item, (list, tuple)) and len(item) >= 1:\n                normalized.append((item[0], item[1] if len(item) > 1 else None))\n            else:\n                normalized.append((str(item), None))\n        except Exception:\n            normalized.append((str(item), None))\n    if use_cache:\n        _RETRIEVAL_CACHE[key] = normalized\n    return normalized\n\n\ndef legal_self_consistency_rag(\n    retriever,\n    decoder,\n    query: str,\n    num_samples: int = 2,\n    temperature: float = 0.5,\n    max_new_tokens: int = 384,\n    top_k: int = 3,\n    use_retrieval_cache: bool = True,\n    dedup_threshold: float = 0.85,\n    enforce_structure: bool = True\n):\n    \"\"\"\n    RAG v·ªõi self-consistency: sinh nhi·ªÅu h∆∞·ªõng l·∫≠p lu·∫≠n, chu·∫©n ho√° sau khi ch·ªçn k·∫øt qu·∫£ t·ªët nh·∫•t.\n    \"\"\"\n    t0 = time.perf_counter()\n    retrieved = _retrieve_with_cache(retriever, query, top_k=top_k, use_cache=use_retrieval_cache)\n    context_text = \"\\n\\n\".join([doc for doc, _ in retrieved])\n    prompt = decoder._build_context(query, context_text)\n\n    # üîπ outputs_raw = ch·ª©a c√°c c√¢u tr·∫£ l·ªùi CH∆ØA format\n    outputs_raw = []\n    print(\"\\nüîÑ ƒêang sinh c√°c h∆∞·ªõng l·∫≠p lu·∫≠n:\")\n\n    for i in range(num_samples):\n        try:\n            out = decoder.decode(prompt, temperature=temperature, max_new_tokens=max_new_tokens, num_samples=1)\n            text_out = out[0].strip() if isinstance(out, (list, tuple)) else str(out).strip()\n            if not text_out:\n                print(f\"‚ö†Ô∏è H∆∞·ªõng {i+1}: Output r·ªóng, b·ªè qua\")\n                continue\n\n            text_out = _clean_model_output(text_out)  # ch·ªâ clean, KH√îNG format\n            outputs_raw.append(text_out)\n            print(f\"‚úÖ H∆∞·ªõng {i+1} ho√†n th√†nh ({len(text_out)} k√Ω t·ª±)\")\n\n        except Exception as e:\n            print(f\"‚ö†Ô∏è L·ªói khi sinh h∆∞·ªõng {i+1}: {e}\")\n\n    if not outputs_raw:\n        return {\"final_answer\": \"Kh√¥ng sinh ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi h·ª£p l·ªá.\", \"candidates\": []}\n\n    # === Self-consistency: ch·ªçn c√¢u tr·∫£ l·ªùi trung b√¨nh v·ªÅ ng·ªØ nghƒ©a ===\n    print(\"\\nüß† ƒêang ƒë√°nh gi√° ƒë·ªô nh·∫•t qu√°n ng·ªØ nghƒ©a...\")\n    embs = _embed_sentences_cached(outputs_raw)\n    sims = util.cos_sim(embs, embs)\n    avg_scores = sims.mean(dim=1)\n    best_idx = int(torch.argmax(avg_scores))\n\n    # üî• Format CH·ªà ·ª©ng vi√™n t·ªët nh·∫•t\n    final_answer = (\n        _format_legal_answer_enhanced(outputs_raw[best_idx])\n        if enforce_structure else outputs_raw[best_idx]\n    )\n\n    # === Format c√°c candidates c√≤n l·∫°i (tu·ª≥ ch·ªçn)\n    candidates = [\n        _format_legal_answer_enhanced(c) if enforce_structure else c\n        for c in outputs_raw\n    ]\n\n    elapsed = time.perf_counter() - t0\n    print(f\"\\n‚è±Ô∏è Ho√†n t·∫•t RAG trong {elapsed:.2f}s ‚Äî ch·ªçn h∆∞·ªõng {best_idx + 1}\")\n\n    return {\"final_answer\": final_answer, \"candidates\": candidates, \"best_idx\": best_idx}\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:58.861687Z","iopub.execute_input":"2025-12-17T10:41:58.861977Z","iopub.status.idle":"2025-12-17T10:41:58.883068Z","shell.execute_reply.started":"2025-12-17T10:41:58.861952Z","shell.execute_reply":"2025-12-17T10:41:58.882410Z"}},"outputs":[{"name":"stdout","text":"Writing legal_self_consistency_rag.py\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!pip install streamlit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:41:58.883784Z","iopub.execute_input":"2025-12-17T10:41:58.884067Z","iopub.status.idle":"2025-12-17T10:42:07.246226Z","shell.execute_reply.started":"2025-12-17T10:41:58.884022Z","shell.execute_reply":"2025-12-17T10:42:07.245225Z"}},"outputs":[{"name":"stdout","text":"Collecting streamlit\n  Downloading streamlit-1.52.1-py3-none-any.whl.metadata (9.8 kB)\nRequirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.3.0)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\nRequirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.5)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.15.0)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.5.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (1.48.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.26.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nDownloading streamlit-1.52.1-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.52.1\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"%%writefile legal_rag_app.py\nimport os\nimport streamlit as st\nimport torch\nimport gc\n\nfrom LegalRetriever import HybridLegalRetriever\nfrom Decoder import Decoder\nfrom build_rag_from_txt_folder import build_rag_from_txt_folder\nfrom legal_self_consistency_rag import legal_self_consistency_rag\n\n# ==========================================\n# ‚öôÔ∏è C·∫•u h√¨nh chung\n# ==========================================\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nst.set_page_config(page_title=\"‚öñÔ∏è Legal RAG Chatbot (Vi·ªát Nam)\", layout=\"centered\")\n\nst.markdown(\n    \"\"\"\n    <style>\n    .chat-message {\n        padding: 0.8em 1.2em;\n        border-radius: 1em;\n        margin-bottom: 0.8em;\n        max-width: 90%;\n        word-wrap: break-word;\n        font-size: 1rem;\n        line-height: 1.5;\n    }\n    .user-msg {\n        background-color: #DCF8C6;\n        margin-left: auto;\n    }\n    .bot-msg {\n        background-color: #F1F0F0;\n        margin-right: auto;\n    }\n    </style>\n    \"\"\",\n    unsafe_allow_html=True\n)\n\nst.title(\"‚öñÔ∏è Legal RAG Chatbot (Lu·∫≠t Vi·ªát Nam)\")\n\n# ==========================================\n# üîπ Kh·ªüi t·∫°o models v√† retriever\n# ==========================================\n@st.cache_resource\ndef load_decoder():\n    \"\"\"Load m√¥ h√¨nh Qwen ƒë√£ fine-tuned lu·∫≠t.\"\"\"\n    decoder = Decoder(\"/kaggle/input/qwen_law_instruct_v3/keras/default/2\")\n    if torch.cuda.is_available():\n        decoder.model.to(\"cuda:0\")\n    return decoder\n\n@st.cache_resource\ndef init_retriever(txt_folder=\"/kaggle/input/luat-txt\"):\n    \"\"\"X√¢y d·ª±ng ch·ªâ m·ª•c t√¨m ki·∫øm lu·∫≠t.\"\"\"\n    docs = build_rag_from_txt_folder(txt_folder)\n    retriever = HybridLegalRetriever()\n    retriever.build_index(docs)\n    return retriever\n\ndecoder = load_decoder()\nretriever = init_retriever()\n\n# ==========================================\n# üí¨ Tr·∫°ng th√°i h·ªôi tho·∫°i\n# ==========================================\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# ==========================================\n# ‚öôÔ∏è Sidebar: ƒëi·ªÅu ch·ªânh tham s·ªë sinh\n# ==========================================\nst.sidebar.header(\"‚öôÔ∏è C·∫•u h√¨nh m√¥ h√¨nh\")\n\ntemperature = st.sidebar.slider(\"Temperature (ng·∫´u nhi√™n)\", 0.0, 1.5, 0.5, 0.05)\nnum_samples = st.sidebar.slider(\"S·ªë m·∫´u sinh (N)\", 1, 6, 3)\ntop_k = st.sidebar.slider(\"Top-K t√†i li·ªáu truy xu·∫•t\", 1, 10, 5)\ntop_p = st.sidebar.slider(\"Top-P (nucleus sampling)\", 0.1, 1.0, 0.9, 0.05)\nrepetition_penalty = st.sidebar.slider(\"Repetition Penalty\", 1.0, 2.0, 1.1, 0.05)\nalpha = st.sidebar.slider(\"Alpha (NLI vs ƒë·ªô d√†i)\", 0.0, 1.0, 0.8, 0.05)\nideal_len = st.sidebar.slider(\"ƒê·ªô d√†i l√Ω t∆∞·ªüng (token)\", 50, 300, 150, 10)\nlen_sigma = st.sidebar.slider(\"ƒê·ªô l·ªách chu·∫©n ƒë·ªô d√†i\", 20, 200, 80, 10)\nmax_new_tokens = st.sidebar.slider(\"Max new tokens\", 50, 2048, 512, 50)\nif st.sidebar.button(\"üßπ Reset h·ªôi tho·∫°i\"):\n    st.session_state.messages = []\n    st.sidebar.success(\"ƒê√£ x√≥a to√†n b·ªô h·ªôi tho·∫°i!\")\n\n# ==========================================\n# üí¨ Giao di·ªán chat ch√≠nh\n# ==========================================\nst.markdown(\"### üí¨ Chatbot ph√°p l√Ω th√¥ng minh\")\n\nquery = st.chat_input(\"Nh·∫≠p c√¢u h·ªèi ph√°p l√Ω c·ªßa b·∫°n...\")\n\n# Hi·ªÉn th·ªã l·∫°i h·ªôi tho·∫°i c≈©\nfor msg in st.session_state.messages:\n    with st.chat_message(msg[\"role\"]):\n        st.markdown(msg[\"content\"])\n\n# ==========================================\n# üöÄ X·ª≠ l√Ω c√¢u h·ªèi m·ªõi\n# ==========================================\nif query:\n    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n    with st.chat_message(\"user\"):\n        st.markdown(query)\n\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"üß† ƒêang truy xu·∫•t v√† t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi...\"):\n            try:\n                answer = legal_self_consistency_rag(  \n                retriever = retriever,\n                decoder = decoder,\n                query = query,\n                num_samples = num_samples,\n                temperature = temperature,\n                max_new_tokens = max_new_tokens,\n                top_k = top_k\n            )\n                reply = answer.get(\"final_answer\", \"Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi ph√π h·ª£p.\")\n            except Exception as e:\n                reply = f\"‚ö†Ô∏è L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}\"\n                torch.cuda.empty_cache()\n                gc.collect()\n\n        st.markdown(reply)\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": reply})\n\n# ==========================================\n# üßπ Cleanup GPU memory\n# ==========================================\ntorch.cuda.empty_cache()\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:42:07.247666Z","iopub.execute_input":"2025-12-17T10:42:07.247979Z","iopub.status.idle":"2025-12-17T10:42:07.256350Z","shell.execute_reply.started":"2025-12-17T10:42:07.247955Z","shell.execute_reply":"2025-12-17T10:42:07.255579Z"}},"outputs":[{"name":"stdout","text":"Writing legal_rag_app.py\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"!pip install pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:42:07.257328Z","iopub.execute_input":"2025-12-17T10:42:07.257574Z","iopub.status.idle":"2025-12-17T10:42:10.588127Z","shell.execute_reply.started":"2025-12-17T10:42:07.257558Z","shell.execute_reply":"2025-12-17T10:42:10.587204Z"}},"outputs":[{"name":"stdout","text":"Collecting pyngrok\n  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.3)\nDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.5.0\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!ngrok config add-authtoken ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:42:10.589354Z","iopub.execute_input":"2025-12-17T10:42:10.589997Z","iopub.status.idle":"2025-12-17T10:42:11.815579Z","shell.execute_reply.started":"2025-12-17T10:42:10.589971Z","shell.execute_reply":"2025-12-17T10:42:11.814742Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from pyngrok import ngrok\nimport threading, os\n\nport = 8501\npublic_url = ngrok.connect(port).public_url\nprint(\"üåê Public URL:\", public_url)\n\ndef run_app():\n    os.system(f\"streamlit run legal_rag_app.py --server.port {port}\")\n\nthreading.Thread(target=run_app).start()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:42:11.816545Z","iopub.execute_input":"2025-12-17T10:42:11.816805Z","iopub.status.idle":"2025-12-17T10:42:12.268591Z","shell.execute_reply.started":"2025-12-17T10:42:11.816774Z","shell.execute_reply":"2025-12-17T10:42:12.267782Z"}},"outputs":[{"name":"stdout","text":"üåê Public URL: https://unsloped-pseudomedievally-makhi.ngrok-free.dev\n\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\n\n  You can now view your Streamlit app in your browser.\n\n  Local URL: http://localhost:8501\n  Network URL: http://172.19.2.2:8501\n  External URL: http://136.112.3.221:8501\n\n","output_type":"stream"},{"name":"stderr","text":"2025-12-17 10:43:50.219017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765968230.243585     187 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765968230.251270     187 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n`torch_dtype` is deprecated! Use `dtype` instead!\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"üìö ƒêang ƒë·ªçc 3 t·ªáp TXT t·ª´: /kaggle/input/luat-txt\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 34.19it/s]\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at ./ms-marco-MiniLM-L-12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nBatches:   0%|          | 0/39 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"‚úÖ T·ªïng s·ªë ƒëo·∫°n vƒÉn b·∫£n (chunks): 1228\nüîπ X√¢y d·ª±ng BM25 index...\nüîπ ƒêang t·∫°o embeddings v√† x√¢y d·ª±ng FAISS index...\n","output_type":"stream"},{"name":"stderr","text":"Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:11<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Ho√†n t·∫•t! ƒê√£ x√¢y d·ª±ng index cho 1,228 ƒëo·∫°n lu·∫≠t v·ªõi vector dim = 768.\n   ‚Ä¢ FAISS vectors: 1228\n   ‚Ä¢ Thi·∫øt b·ªã reranker: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 34.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"üîé Truy v·∫•n: n·ªØ ƒë·ªß bao nhi√™u tu·ªïi th√¨ ƒë∆∞·ª£c ph√©p ƒëƒÉng k√Ω k·∫øt h√¥n\nüìë Top 5 k·∫øt qu·∫£:\n   01. (0.4860) ƒêi·ªÅu 8. ƒêi·ªÅu ki·ªán k·∫øt h√¥n [KHO·∫¢N] 1. Nam, n·ªØ k·∫øt h√¥n v·ªõi nhau ph·∫£i tu√¢n theo c√°c ƒëi·ªÅu ki·ªán sau ƒë√¢y: a) Nam t·ª´ ƒë·ªß 20 tu·ªïi tr·ªü l√™n, n·ªØ t·ª´ ƒë·ªß 18 tu·ªïi tr·ªü l√™n; b) Vi·ªác k·∫øt h√¥n do nam v√† n·ªØ t·ª± nguy·ªán quy·∫øt...\n   02. (0.4848) ƒêi·ªÅu 14. Gi·∫£i quy·∫øt h·∫≠u qu·∫£ c·ªßa vi·ªác nam, n·ªØ chung s·ªëng v·ªõi nhau nh∆∞ v·ª£ ch·ªìng m√† kh√¥ng ƒëƒÉng k√Ω k·∫øt h√¥n [KHO·∫¢N] 1. Nam, n·ªØ c√≥ ƒë·ªß ƒëi·ªÅu ki·ªán k·∫øt h√¥n theo quy ƒë·ªãnh c·ªßa Lu·∫≠t n√†y chung s·ªëng v·ªõi nhau nh∆∞ v·ª£ ...\n   03. (0.4844) ƒêi·ªÅu 13. X·ª≠ l√Ω vi·ªác ƒëƒÉng k√Ω k·∫øt h√¥n kh√¥ng ƒë√∫ng th·∫©m quy·ªÅn Trong tr∆∞·ªùng h·ª£p vi·ªác ƒëƒÉng k√Ω k·∫øt h√¥n kh√¥ng ƒë√∫ng th·∫©m quy·ªÅn th√¨ khi c√≥ y√™u c·∫ßu, c∆° quan nh√† n∆∞·ªõc c√≥ th·∫©m quy·ªÅn thu h·ªìi, h·ªßy b·ªè gi·∫•y ch·ª©ng nh·∫≠n...\n   04. (0.4844) ƒêi·ªÅu 16 c·ªßa Lu·∫≠t n√†y. [KHO·∫¢N] 2. Trong tr∆∞·ªùng h·ª£p nam, n·ªØ chung s·ªëng v·ªõi nhau nh∆∞ v·ª£ ch·ªìng theo quy ƒë·ªãnh t·∫°i kho·∫£n 1 ƒêi·ªÅu n√†y nh∆∞ng sau ƒë√≥ th·ª±c hi·ªán vi·ªác ƒëƒÉng k√Ω k·∫øt h√¥n theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t th...\n   05. (0.4839) ƒêi·ªÅu 9. ƒêƒÉng k√Ω k·∫øt h√¥n [KHO·∫¢N] 1. Vi·ªác k·∫øt h√¥n ph·∫£i ƒë∆∞·ª£c ƒëƒÉng k√Ω v√† do c∆° quan nh√† n∆∞·ªõc c√≥ th·∫©m quy·ªÅn th·ª±c hi·ªán theo quy ƒë·ªãnh c·ªßa Lu·∫≠t n√†y v√† ph√°p lu·∫≠t v·ªÅ h·ªô t·ªãch. Vi·ªác k·∫øt h√¥n kh√¥ng ƒë∆∞·ª£c ƒëƒÉng k√Ω the...\n\nüîÑ ƒêang sinh c√°c h∆∞·ªõng l·∫≠p lu·∫≠n:\n\nüß† H∆∞·ªõng #1 (ƒë·ªô d√†i: 269 k√Ω t·ª±)\n{'Lƒ©nh v·ª±c': 'Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh', 'CƒÉn c·ª© ph√°p l√Ω': 'Kho·∫£n a ƒêi·ªÅu 8', 'Ph√¢n t√≠ch': 'Nam t·ª´ ƒë·ªß 20 tu·ªïi tr·ªü l√™n, n·ªØ t·ª´ ƒë·ªß 18 tu·ªïi tr·ªü l√™n m·ªõi ƒë√°p ·ª©ng ƒëi·ªÅu ki·ªán v·ªÅ ƒë·ªô tu·ªïi ƒë·ªÉ ƒëƒÉng k√Ω k·∫øt h√¥n.', 'K·∫øt lu·∫≠n': 'N·ªØ t·ª´ ƒë·ªß 18 tu·ªïi tr·ªü l√™n m·ªõi ƒë∆∞·ª£c ph√©p ƒëƒÉng k√Ω k·∫øt h√¥n.","output_type":"stream"},{"name":"stderr","text":"Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 93.98it/s]\nBatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 98.48it/s]\n","output_type":"stream"}],"execution_count":26}]}